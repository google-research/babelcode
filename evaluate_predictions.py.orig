"""Script to execute code for evaluation across multiple languages."""
# pylint: disable=g-bare-generic
# pylint: disable=g-importing-member
# pylint: disable=raise-missing-from
# pylint: disable=logging-fstring-interpolation
import json
import random
import shutil
from collections import defaultdict
from pathlib import Path
from typing import Optional

import gin
import tensorflow as tf
from absl import app
from absl import flags
from absl import logging

from babelcode import utils
from babelcode.languages import LanguageRegistry
from babelcode.run_execution import run_execution_for_lang_predictions


@gin.configurable('general',
                  denylist=[
                      'experiment_name', 'prediction_path', 'output_path',
                      'test_code_path', 'overwrite'
                  ])
def execute_predictions_from_file(experiment_name: str,
                                  prediction_path: Path,
                                  output_path: Path,
                                  test_code_path: Path,
                                  overwrite: bool,
                                  debug: bool = False,
                                  debug_code_gen_dir: Optional[str] = None,
                                  debug_num_preds: int = -1,
                                  debug_lang: str = Optional[str],
                                  seed: int = 1,
                                  validation_mode: bool = False):
  """Evaluate a set of predictions.

  Take in a set of predictions files, and execute them against test cases.
  Results are saved to a `results.json` file in the `output_path`.


  Args:
    experiment_name:
    prediction_path:
    output_path:
    test_code_path:
    overwrite:
    debug:
    debug_code_gen_dir:
    debug_num_preds:
    seed:

  Raises:
    ValueError: Output exists and not overwriting
    FileNotFoundError: Testing code could not be found.
  """

  debug = debug_num_preds > 0 or debug
  if debug:
    experiment_name = f'{experiment_name}.DEBUG'
    print('Debug is enabled.')
  output_path = output_path.joinpath(experiment_name)
  if output_path.exists():
    if not overwrite:
      raise ValueError(f'{output_path} exists and overwrite is not enabled')
    shutil.rmtree(output_path)
  output_path.mkdir(exist_ok=True, parents=True)
  utils.setup_logging('logs', debug, log_path=output_path)
  logging.info(f'Evaluating predictions from {prediction_path}')
  logging.info(f'Saving to {output_path}')
  logging.info(f'Reading tests from {test_code_path}')

  summary_writer = tf.summary.create_file_writer(str(output_path),
                                                 flush_millis=120)

  # Allow the use of multiple language predictions in a single file by using
  # the language in the key during reading.
  key_fn = lambda l: f'{l["language"]}/{l["qid"]}_{l["id"]}'
  all_predictions = utils.jsonl_file_to_map(prediction_path, key_fn)
  logging.info(f'Found {len(all_predictions)} total predictions')

  # Separate the predictions into their own language buckets.
  preds_by_lang = defaultdict(dict)
  logging.debug('Grouping predictions by language')
  for full_key, value in all_predictions.items():
    language, key = full_key.split('/')
    preds_by_lang[language][key] = value

  langs_found = list(sorted(preds_by_lang))
  if debug_lang:
    langs_found = [debug_lang]
  logging.info(f'{len(langs_found)} language(s) found')

  logging.info('# Preds by Language:')
  for lang in langs_found:
    logging.info(f'{lang:>10} = {len(preds_by_lang[lang])}')

  # Save the gin config and the information on where the predictions and test
  # code are located to two files in the output directory.
  logging.info('Saving launch information...')
  with output_path.joinpath('config.gin').open('w') as f:
    f.write(gin.config_str())

  with output_path.joinpath('launch_info.json').open('w') as f:
    json.dump(
        {
            'prediction_path': str(prediction_path),
            'test_code_path': str(test_code_path),
        },
        f,
        indent=True)

  logging.info('Reading questions from %s', test_code_path)
  question_mapping = defaultdict(dict)
  found = 0
  for line in map(json.loads,
                  test_code_path.joinpath('testing_code.jsonl').open()):
    question_mapping[line['language']][str(line['qid'])] = line
    found += 1

  logging.info('Found %d questions across %d languages', found,
               len(question_mapping))

  all_metrics = {}
  all_pred_metrics = []
  all_error_per_lang = {}
  for lang_name in langs_found:
    lang = LanguageRegistry.get_language(lang_name)

    if debug_num_preds > 0 and len(preds_by_lang[lang_name]) > debug_num_preds:
      logging.warning(f'ONLY USING {debug_num_preds} PREDICTIONS')

      # Set the seed for debugging a subset.
      utils.set_seed(seed)
      to_keep = random.sample(list(preds_by_lang[lang_name]), debug_num_preds)
      preds_to_use = {
          k: v for k, v in preds_by_lang[lang_name].items() if k in to_keep
      }
    else:
      preds_to_use = preds_by_lang[lang_name]

    metrics, pred_results = run_execution_for_lang_predictions(
        lang=lang,
        question_mapping=question_mapping[lang_name],
        raw_predictions=preds_to_use,
        output_path=output_path,
        debug_dir_path=debug_code_gen_dir,  # type: ignore
        seed=seed,
        step=_STEP.value,
        summary_writer=summary_writer,
        force_question_entry=_FORCE_QUESTION_ENTRY.value)  # type:ignore

    # Add the number of questions with all predictions having error.
    with_all_error = []
    with_all_timed_out = []
    num_questions = 0
    for k, metric_dict in metrics.items():
      # Skip the overview metrics.
      if 'metrics' in k:
        continue
      num_preds = metric_dict['num_predictions']
      qid = k.split('/')[-1]
      title = question_mapping[lang_name][qid]['title']
      if num_preds == metric_dict['Had Error']:
        with_all_error.append(f'{title} ({qid=})')
      elif num_preds == metric_dict['Timed Out']:
        with_all_timed_out.append(f'{title} ({qid=})')
      num_questions += 1
    all_error_per_lang[lang_name] = (with_all_error, with_all_timed_out,
                                     num_questions)
    all_metrics.update(metrics)
    all_pred_metrics.extend(pred_results)
  metric_path = output_path.joinpath('metrics.json')
  logging.info('Saving metrics to %s', metric_path)
  with metric_path.open('w') as f:
    json.dump(all_metrics, f)

  pred_result_path = output_path.joinpath('pred_results.jsonl')
  logging.info('Saving all prediction results to %s', pred_result_path)
  with pred_result_path.open('w') as f:
    for l in all_pred_metrics:
      f.write(json.dumps(l.to_dict()) + '\n')
  if validation_mode:
    logging.info('Number of Questions With Issues For %d languages:',
                 len(all_error_per_lang))
    for lang_name, (with_error, with_timeout,
                    num_questions) in all_error_per_lang.items():
      msg = f'{lang_name:>16} = {len(with_error)+len(with_timeout)}/{num_questions}'
      logging.info(msg)
      logging.info('%s\tWith Error=%s', ' ' * 16, with_error)

      logging.info('%s\tWith Timeout=%s', ' ' * 16, with_timeout)
  else:
    logging.info('Metrics:')
    for lang_name in langs_found:
      logging.info('\t%s:', lang_name)
      metrics = all_metrics[f'{lang_name}/metrics']
      to_print = ['questions_passed']
      for k in metrics:
        if 'subsampling_pass' in k or 'estimate' in k:
          to_print.append(k)
      for k in sorted(to_print):
        value = metrics[k]
        key_str = f'{k:>32}'
        if isinstance(value, float):
          value_str = f'{value:.3f}'
        elif isinstance(value, (dict, list, tuple)):
          continue
        else:
          value_str = f'{value}'
        logging.info('\t%s = %s', key_str, value_str)


if __name__ == '__main__':
  FLAGS = flags.FLAGS
  _GIN = flags.DEFINE_string('gin_file', None, 'Gin configuration file.')
  _EXP_NAME = flags.DEFINE_string('experiment_name', None,
                                  'Name of the experiment.')
  _OUTPUT_PATH = flags.DEFINE_string('output_path', None, 'Output path.')
  _PRED_PATH = flags.DEFINE_string('predictions', None, 'Prediction path.')
  _TEST_CODE_PATH = flags.DEFINE_string('test_code', None,
                                        'Path to testing code')
  _OVERWRITE = flags.DEFINE_bool('overwrite', False,
                                 'Overwrite output file if it exists')
  _DEBUG = flags.DEFINE_bool('debug', False, 'Enable Debug mode')
  _NUM_CORES = flags.DEFINE_integer('cpu_count',
                                    None,
                                    help='Number of CPUs to use.')
  _DEBUG_NUM_PREDS = flags.DEFINE_integer(
      'debug_num_preds', -1, help='Debugging number of predictions to use.')
  _PREDS_PER_QUESTION = flags.DEFINE_integer(
      'samples', None, 'Number of predictions per question.')
  _STEP = flags.DEFINE_integer('step', 0, 'Step to use for tensorboard.')
  _FORCE_QUESTION_ENTRY = flags.DEFINE_bool(
      'use_question_entry', False,
      'Force using the question entry points instead of the prediction ones.')
  _VALIDATION_MODE = flags.DEFINE_bool('validation', False,
                                       'Enable validation printing.')
  _DEBUG_LANG = flags.DEFINE_string('debug_lang', None, "Language to debug.")

  def eval_preds_main(_):
    """Main entry point to the launch."""
    FLAGS['alsologtostderr'].value = True
    # Create gin bindings to overwrite the config.
    bindings = []

    if _DEBUG.value:
      bindings.append('general.debug=True')
    if _NUM_CORES.value:
      bindings.append(f'execution.num_workers={_NUM_CORES.value}')
    if _DEBUG_NUM_PREDS.value > 0:
      bindings.append(f'general.debug_num_preds={_DEBUG_NUM_PREDS.value}')
    if _PREDS_PER_QUESTION.value is not None and _PREDS_PER_QUESTION.value > 0:
      bindings.append(
          f'metrics.num_preds_per_question={_PREDS_PER_QUESTION.value}')

    print(f'gin_{bindings=}')
    gin_path = Path(_GIN.value).resolve()
    print(f'Gin Path={gin_path}')
    gin.parse_config_file(str(gin_path))
    gin.parse_config(bindings=bindings)

    execute_predictions_from_file(
        experiment_name=_EXP_NAME.value,
        prediction_path=Path(_PRED_PATH.value).resolve(),
        output_path=Path(_OUTPUT_PATH.value).resolve(),
        test_code_path=Path(_TEST_CODE_PATH.value).resolve(),
        overwrite=_OVERWRITE.value,
        validation_mode=_VALIDATION_MODE.value,
        debug_lang=_DEBUG_LANG.value)

  flags.mark_flags_as_required([
      _GIN.name, _EXP_NAME.name, _OUTPUT_PATH.name, _PRED_PATH.name,
      _TEST_CODE_PATH.name
  ])
  app.run(eval_preds_main)
